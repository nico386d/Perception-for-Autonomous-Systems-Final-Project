{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "971f285b",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "660a7808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: C:\\Users\\nico3\\.cache\\kagglehub\\datasets\\klemenko\\kitti-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "root_path = Path(kagglehub.dataset_download(\"klemenko/kitti-dataset\")).resolve()\n",
    "print(\"Dataset downloaded to:\", root_path)\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9034f31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of objects: 32037\n",
      "Example sample: {'image_path': WindowsPath('C:/Users/nico3/.cache/kagglehub/datasets/klemenko/kitti-dataset/versions/1/data_object_image_2/training/image_2/000000.png'), 'class_id': 0, 'bbox': [712.4, 143.0, 810.73, 307.92]}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import kagglehub\n",
    "\n",
    "\n",
    "# Download dataset and point to train folders\n",
    "#root_path = Path(kagglehub.dataset_download(\"klemenko/kitti-dataset\")).resolve()\n",
    "image_train_folder = root_path / \"data_object_image_2\" / \"training\" / \"image_2\"\n",
    "label_train_folder = root_path / \"data_object_label_2\" / \"training\" / \"label_2\"\n",
    "\n",
    "# List image and label files once\n",
    "image_files = sorted(image_train_folder.glob(\"*.png\"))\n",
    "label_files = sorted(label_train_folder.glob(\"*.txt\"))\n",
    "\n",
    "# Keep only these classes\n",
    "class_keep = {\"Pedestrian\": 0, \"Cyclist\": 1, \"Car\": 2}\n",
    "\n",
    "def parse_label_file(path):\n",
    "    objects = []\n",
    "    for line in open(path):\n",
    "        parts = line.split()\n",
    "        if not parts:\n",
    "            continue  # If the line is empty we skip\n",
    "\n",
    "        # If the line dosn't contain Pedestrian/Cyclist/Car then skip\n",
    "        cls = parts[0]\n",
    "        if cls not in class_keep:\n",
    "            continue\n",
    "\n",
    "        # if too truncated(how much of the object is cut off at the image border) / occluded (how much the object is blocked by others)\n",
    "        trunc, occ = float(parts[1]), int(parts[2])\n",
    "        if trunc > 0.7 or occ > 2:\n",
    "            continue\n",
    "              \n",
    "        x1, y1, x2, y2 = map(float, parts[4:8])\n",
    "        objects.append({\n",
    "            \"class_name\": cls,\n",
    "            \"class_id\": class_keep[cls],\n",
    "            \"bbox\": [x1, y1, x2, y2],\n",
    "        })\n",
    "    return objects\n",
    "\n",
    "# Parse all label files\n",
    "label_data = [parse_label_file(p) for p in label_files]\n",
    "\n",
    "# Build list of object samples\n",
    "samples = [\n",
    "    {\n",
    "        \"image_path\": img_path,\n",
    "        \"class_id\": obj[\"class_id\"],\n",
    "        \"bbox\": obj[\"bbox\"],\n",
    "    }\n",
    "    for img_path, objects in zip(image_files, label_data)\n",
    "    for obj in objects\n",
    "]\n",
    "\n",
    "print(\"Total number of objects:\", len(samples))\n",
    "print(\"Example sample:\", samples[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2cba05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class KittiCropClassificationDataset(Dataset):\n",
    "    def __init__(self, samples, transform=None):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        image = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n",
    "        x1, y1, x2, y2 = sample[\"bbox\"]\n",
    "        image = image.crop((x1, y1, x2, y2))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        label = sample[\"class_id\"]\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f271c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using total samples: 32037\n",
      "Train samples: 1200\n",
      "Val samples:   300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1200, 300)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torchvision as tv\n",
    "\n",
    "# KITTI full resolution\n",
    "# the original image is 1242x375, but we are only using crops of the bounding boxes so 244x244 is sufficient\n",
    "full_width  = 244 #1242\n",
    "full_height = 244 #375\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 5\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-4\n",
    "max_samples = 1500 # Set to a lower number for quick testing, e.g., 5000\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(samples)\n",
    "\n",
    "if len(samples) > max_samples:\n",
    "    max_samples = samples[:max_samples]\n",
    "\n",
    "print(\"Using total samples:\", len(samples))\n",
    "# -----------------------------\n",
    "\n",
    "# 80/20 train/val split\n",
    "split_index = int(0.8 * len(max_samples))\n",
    "train_samples = max_samples[:split_index]\n",
    "val_samples   = max_samples[split_index:]\n",
    "\n",
    "print(\"Train samples:\", len(train_samples))\n",
    "print(\"Val samples:  \", len(val_samples))\n",
    "\n",
    "# -----------------------------\n",
    "# Transforms\n",
    "# -----------------------------\n",
    "train_transform = tv.transforms.Compose([\n",
    "    tv.transforms.Resize((full_height, full_width)),    \n",
    "    tv.transforms.RandomHorizontalFlip(),\n",
    "    tv.transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                            [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = tv.transforms.Compose([\n",
    "    tv.transforms.Resize((full_height, full_width)),\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                            [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = KittiCropClassificationDataset(train_samples, transform=train_transform)\n",
    "val_dataset   = KittiCropClassificationDataset(val_samples,   transform=val_transform)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "len(train_dataset), len(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be2d7d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA device name:  NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "print(\"CUDA device name: \", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n",
    "\n",
    "num_classes = len(class_keep)\n",
    "\n",
    "\n",
    "#weights = models.ResNet50_Weights.IMAGENET1K_V2\n",
    "#model = models.resnet50(weights=weights)\n",
    "#model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "weights = models.VGG16_Weights.IMAGENET1K_V1\n",
    "model = models.vgg16(weights=weights)\n",
    "in_features = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(in_features, num_classes)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0533f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a97a2d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, loss_function, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for images, labels in tqdm(data_loader, desc=f\"Training Epoch {epoch}\", leave=False):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        total_samples += images.size(0)\n",
    "\n",
    "    average_loss = total_loss / max(1, total_samples)\n",
    "    return average_loss\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_classifier(model, data_loader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    for images, labels in tqdm(data_loader, desc=\"Validating\", leave=False):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        predicted_labels = outputs.argmax(dim=1)\n",
    "\n",
    "        all_predictions.extend(predicted_labels.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    macro_f1 = f1_score(all_labels, all_predictions, average=\"macro\")\n",
    "    return accuracy, macro_f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e82e383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20af6e7d89884481a90c98cb9d9467dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f4a3ef69534b1fba1a6462f9ced0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.2059 | val_acc=0.963 | val_f1=0.636\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856858cba59d4cd9830ede427e2279c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ece5d68d7ec43d18be9b746e320c129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | train_loss=0.1480 | val_acc=0.960 | val_f1=0.763\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa04ad01d5eb478e80cfd524727873a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m best_validation_accuracy = \u001b[32m0.0\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     training_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     validation_accuracy, validation_macro_f1 = evaluate_classifier(model, val_loader)\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrain_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mval_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalidation_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mval_f1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalidation_macro_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, data_loader, optimizer, loss_function, epoch)\u001b[39m\n\u001b[32m     15\u001b[39m     loss.backward()\n\u001b[32m     16\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * images.size(\u001b[32m0\u001b[39m)\n\u001b[32m     19\u001b[39m     total_samples += images.size(\u001b[32m0\u001b[39m)\n\u001b[32m     21\u001b[39m average_loss = total_loss / \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, total_samples)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=learning_rate,\n",
    "                              weight_decay=weight_decay)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "best_validation_f1 = 0.0\n",
    "best_validation_accuracy = 0.0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    training_loss = train_one_epoch(model, train_loader, optimizer, loss_function, epoch)\n",
    "    validation_accuracy, validation_macro_f1 = evaluate_classifier(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train_loss={training_loss:.4f} | \"\n",
    "          f\"val_acc={validation_accuracy:.3f} | \"\n",
    "          f\"val_f1={validation_macro_f1:.3f}\")\n",
    "\n",
    "    if validation_macro_f1 > best_validation_f1:\n",
    "        best_validation_f1 = validation_macro_f1\n",
    "        best_validation_accuracy = validation_accuracy\n",
    "\n",
    "print(\"\\nBest validation accuracy:\", best_validation_accuracy)\n",
    "print(\"Best validation macro F1:\", best_validation_f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu02452",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
